{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9670042,"sourceType":"datasetVersion","datasetId":5909190}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.preprocessing import MinMaxScaler\nfrom copy import deepcopy as dc\n\n# Check device availability\nis_cuda = torch.cuda.is_available()\ndevice = torch.device(\"cuda\" if is_cuda else \"cpu\")\nprint(\"Using device:\", device)\n\n# Load data for all coins\n\ncoin_names = ['ENJ','ETP','FUEL','GBP','HSC','KIN','KMD','LEND','MBL','NEST','NPXS','OCEAN','ONL','QUN','RSR','SFG','SWAP','TFUEL','TNT','UTK','WXT','XSR','YEE','YFII','ZEN','AGI','AUC','BCN','BNT','BZRX','CELR','CGLD','CHAT','CHR','CKB','DAI','DIA','DMG','DOTUP','DX']\n\n\n# Initialize an empty DataFrame to hold all data\nall_data = []\n\n# Iterate over each coin name and load data\nfor coin in coin_names:\n    file_path = f'/kaggle/input/multimodalai-prediction/public_dataset/train/price_data/{coin}.csv'\n    df = pd.read_csv(file_path)\n    \n    # Extract relevant columns and append to all_data list\n    data = df[['datetime', 'close_x']]\n    data['Date'] = data['datetime']\n    \n    # Prepare DataFrame for LSTM\n    lookback = 5\n    shifted_df = prepare_dataframe_for_lstm(data, lookback)\n    \n    all_data.append(shifted_df)\n\n# Concatenate all DataFrames into a single DataFrame\nfinal_data = pd.concat(all_data, ignore_index=True)\n\n# Prepare data for LSTM model training\nshifted_df_as_np = final_data.to_numpy()\nscaler = MinMaxScaler(feature_range=(-1, 1))\nshifted_df_as_np = scaler.fit_transform(shifted_df_as_np)\n\nX = shifted_df_as_np[:, 1:]\nX = dc(np.flip(X, axis=1))  # Flip according to time order\ny = shifted_df_as_np[:, 0]\n\nX = X.reshape((-1, lookback, 1))\ny = y.reshape((-1, 1))\n\nX = torch.tensor(X).float().to(device)\ny = torch.tensor(y).float().to(device)\n\n# Create Dataset and DataLoader for training on all data\nclass TimeSeriesDataset(Dataset):\n    def _init_(self, X, y):\n        self.X = X\n        self.y = y\n\n    def _len_(self):\n        return len(self.X)\n\n    def _getitem_(self, i):\n        return self.X[i], self.y[i]\n\ntrain_dataset = TimeSeriesDataset(X, y)\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n\n# Define LSTM model architecture\nclass LSTM(nn.Module):\n    def _init_(self, input_size=1, hidden_size=20, num_stacked_layers=2):\n        super()._init_()\n        self.lstm = nn.LSTM(input_size, hidden_size, num_stacked_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size, 1)\n\n    def forward(self, x):\n        h0 = torch.zeros(num_stacked_layers, x.size(0), hidden_size).to(device)\n        c0 = torch.zeros(num_stacked_layers, x.size(0), hidden_size).to(device)\n        \n        out, _ = self.lstm(x, (h0, c0))\n        return self.fc(out[:, -1])\n\n# Initialize model and training parameters\nmodel = LSTM().to(device)\nloss_function = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n\ntrain_losses = []\nnum_epochs = 30\n\ndef train_one_epoch():\n    model.train()\n    running_loss = 0.0\n    \n    for batch in train_loader:\n        x_batch, y_batch = batch[0].to(device), batch[1].to(device)\n        \n        optimizer.zero_grad()\n        output = model(x_batch)\n        loss = loss_function(output, y_batch)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n\n    avg_loss_across_batches = running_loss / len(train_loader)\n    train_losses.append(avg_loss_across_batches)\n    print(f'Average train loss: {avg_loss_across_batches:.3f}')\n\n# Training loop\nfor epoch in range(num_epochs):\n    print(f'Epoch: {epoch + 1}')\n    train_one_epoch()\n\n# Predictions on the entire dataset after training\nwith torch.no_grad():\n    predictions = model(X).cpu().numpy()\n\n# Inverse transform predictions to original scale\ndummies = np.zeros((X.shape[0], lookback + 1))\ndummies[:, 0] = predictions.flatten()\ndummies = scaler.inverse_transform(dummies)\n\npredictions_in_original_scale = dummies[:, 0]\n\n# Plotting results\nplt.plot(final_data['close_x'].values[lookback:], label='Actual Close')\nplt.plot(predictions_in_original_scale, label='Predicted Close')\nplt.xlabel('Day')\nplt.ylabel('Close')\nplt.title('Prediction on Entire Dataset')\nplt.legend()\nplt.show()\n\n# Plotting training losses\nplt.plot(train_losses, label='Train Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.title('Training Loss Over Epochs')\nplt.show()\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}